{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73feed4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/compu/anaconda3/envs/eagle2/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import requests\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d558056",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c287d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant \"\n",
    "    \"first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning \"\n",
    "    \"process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., \"\n",
    "    \"<think> reasoning process here </think><answer> answer here </answer>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc82fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_eagle_multiview_conversation(example, processor):\n",
    "    prompt_blocks = example[\"prompt_blocks\"]\n",
    "    # Change the value of 'type' from 'image_url' to 'image' in dicts\n",
    "    for block in prompt_blocks:\n",
    "        if isinstance(block, dict) and block.get(\"type\") == \"image_url\":\n",
    "            block[\"type\"] = \"image\"    \n",
    "    \n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt_blocks,\n",
    "        },\n",
    "    ]\n",
    "    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        # \"image\": example[\"image\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0333aebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_conversation(example, processor):\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": example[\"problem\"]},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"image\": example[\"image\"],\n",
    "        \"solution\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6e8ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"lmms-lab/multimodal-open-r1-8k-verified\"\n",
    "dataset = load_dataset(dataset_id, split=\"train[:5%]\")\n",
    "\n",
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "test_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768e288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"nvidia/Eagle2-2B\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"nvidia/Eagle2-2B\", trust_remote_code=True, use_fast=True)\n",
    "processor.tokenizer.padding_side = \"left\"\n",
    "\n",
    "train_dataset = train_dataset.map(make_conversation, processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441a9301",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset[0][\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6a5b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9195d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70d870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def format_reward(completions, **kwargs):\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"^<think>\\n.*?\\n</think>\\n<answer>\\n.*?\\n</answer>$\"\n",
    "    matches = [re.match(pattern, content, re.DOTALL | re.MULTILINE) for content in completions]\n",
    "    rewards = [1.0 if match else 0.0 for match in matches]\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1d6a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_verify import LatexExtractionConfig, parse, verify\n",
    "from latex2sympy2_extended import NormalizationConfig\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def accuracy_reward(completions: list[list[dict[str, str]]], solution: list[str], **kwargs) -> list[Optional[float]]:\n",
    "    \"\"\"Reward function that checks if the completion matches the ground truth.\n",
    "    - If both gold and prediction are parseable → use math verification.\n",
    "    - If not parseable → compare as normalized text.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "\n",
    "    for completion, sol in zip(completions, solution):\n",
    "        try:\n",
    "            gold_parsed = parse(sol, extraction_mode=\"first_match\")\n",
    "        except Exception as e:\n",
    "            gold_parsed = []\n",
    "\n",
    "        if len(gold_parsed) != 0:\n",
    "            # Try parsing predicted answer too\n",
    "            try:\n",
    "                answer_parsed = parse(\n",
    "                    completion,\n",
    "                    extraction_config=[\n",
    "                        LatexExtractionConfig(\n",
    "                            normalization_config=NormalizationConfig(\n",
    "                                nits=False,\n",
    "                                malformed_operators=False,\n",
    "                                basic_latex=True,\n",
    "                                boxed=\"all\",\n",
    "                                units=True,\n",
    "                            ),\n",
    "                            boxed_match_priority=0,\n",
    "                            try_extract_without_anchor=False,\n",
    "                        )\n",
    "                    ],\n",
    "                    extraction_mode=\"first_match\",\n",
    "                )\n",
    "                reward = float(verify(gold_parsed, answer_parsed))\n",
    "            except Exception as e:\n",
    "                print(f\"verify failed: {e}, answer: {completion}, gold: {sol}\")\n",
    "                reward = None\n",
    "        else:\n",
    "            # fallback to text match\n",
    "            reward = float(completion.strip().lower() == sol.strip().lower())\n",
    "\n",
    "        rewards.append(reward)\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee56757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig\n",
    "\n",
    "# Configure training arguments using GRPOConfig\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=\"Qwen2.5-VL-3B-Instruct-Thinking\",\n",
    "    learning_rate=1e-5,\n",
    "    remove_unused_columns=False,  # to access the solution column in accuracy_reward\n",
    "    num_train_epochs=1,\n",
    "    bf16=True,\n",
    "    # Parameters that control the data preprocessing\n",
    "    per_device_train_batch_size=2,\n",
    "    max_completion_length=1024,  # default: 256\n",
    "    num_generations=2,  # default: 8\n",
    "    max_prompt_length=2048,\n",
    "    # Parameters related to reporting and saving\n",
    "    report_to=[\"wandb\"],\n",
    "    logging_steps=10,\n",
    "    push_to_hub=True,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dbcc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOTrainer\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=processor,\n",
    "    reward_funcs=[format_reward, accuracy_reward],\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcd7515",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c373c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(training_args.output_dir)\n",
    "trainer.push_to_hub(dataset_name=dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77009f71",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebe3b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "\n",
    "\n",
    "def clear_memory():\n",
    "    # Delete variables if they exist in the current global scope\n",
    "    if \"inputs\" in globals():\n",
    "        del globals()[\"inputs\"]\n",
    "    if \"model\" in globals():\n",
    "        del globals()[\"model\"]\n",
    "    if \"processor\" in globals():\n",
    "        del globals()[\"processor\"]\n",
    "    if \"trainer\" in globals():\n",
    "        del globals()[\"trainer\"]\n",
    "    if \"peft_model\" in globals():\n",
    "        del globals()[\"peft_model\"]\n",
    "    if \"bnb_config\" in globals():\n",
    "        del globals()[\"bnb_config\"]\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Garbage collection and clearing CUDA memory\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    time.sleep(2)\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "\n",
    "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9159e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_id = \"sergiopaniego/Qwen2.5-VL-3B-Instruct-Thinking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d1ac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "\n",
    "trained_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    trained_model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "trained_processor = AutoProcessor.from_pretrained(trained_model_id, use_fast=True, padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779128b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "\n",
    "def generate_with_reasoning(problem, image):\n",
    "    # Conversation setting for sending to the model\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"text\", \"text\": problem},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    prompt = trained_processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "    # Process images using the process_vision_info from qwen_vl_utils\n",
    "    image_inputs, video_inputs = process_vision_info(conversation)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[prompt],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(trained_model.device)\n",
    "\n",
    "    # Generate text without gradients\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        output_ids = trained_model.generate(**inputs, max_new_tokens=500)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Decode and extract model response\n",
    "    generated_text = trained_processor.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Get inference time\n",
    "    inference_duration = end_time - start_time\n",
    "\n",
    "    # Get number of generated tokens\n",
    "    num_input_tokens = inputs[\"input_ids\"].shape[1]\n",
    "    num_generated_tokens = output_ids.shape[1] - num_input_tokens\n",
    "\n",
    "    return generated_text, inference_duration, num_generated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4040d04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text, inference_duration, num_generated_tokens = generate_with_reasoning(\n",
    "    test_dataset[0][\"problem\"], test_dataset[0][\"image\"]\n",
    ")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dac591",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[0][\"solution\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eagle2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
