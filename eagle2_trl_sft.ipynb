{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73feed4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/compu/anaconda3/envs/eagle2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import requests\n",
    "\n",
    "from PIL import Image\n",
    "import torch.distributed as dist\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "\n",
    "# NOTE: \n",
    "# For GPU memory optimization, before starting, go to the Hugging Face cache and set \"max_dynamic_tiles\" from 12 to 1 in the config.json and preprocessor_config.json file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d558056",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de8c535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_processed_datasets = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dd99ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ealge_format_multiview_data(sample):\n",
    "    prompt_blocks = sample[\"prompt_blocks\"]\n",
    "    # Change the value of 'type' from 'image_url' to 'image' in dicts\n",
    "    for block in prompt_blocks:\n",
    "        if isinstance(block, dict) and block.get(\"type\") == \"image_url\":\n",
    "            block[\"type\"] = \"image\"\n",
    "            \n",
    "    answer = sample[\"ground_truth_answer\"]\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt_blocks\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": answer}],\n",
    "            },\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a39488fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ealge_format_data(sample):\n",
    "    return {\n",
    "        \"images\": [sample[\"image\"]],\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"image\": sample[\"image\"],\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": sample[\"query\"],\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": sample[\"label\"][0]}],\n",
    "            },\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bccf6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(sample):\n",
    "    return {\n",
    "        # \"images\": [sample[\"image\"]],\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    # {\n",
    "                    #     \"type\": \"image\",\n",
    "                    #     \"image\": sample[\"image\"],\n",
    "                    #     # \"image\": \"https://www.ilankelman.org/stopsigns/australia.jpg\",\n",
    "                    # },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": sample[\"query\"],\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": sample[\"label\"][0]}],\n",
    "            },\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41ee4c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 0 samples with None image from train_dataset\n",
      "time taken (train_dataset to list) :  17.862751007080078\n",
      "train_dataset length:  2830\n",
      "time taken (train_dataset to Dataset) :  0.015305757522583008\n",
      "eval_dataset length:  192\n",
      "time taken (eval_dataset to Dataset) :  0.8748865127563477\n",
      "test_dataset length:  250\n",
      "time taken (test_dataset to Dataset) :  0.7205157279968262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2830/2830 [00:00<00:00, 124071.08 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 192/192 [00:00<00:00, 54537.88 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 250/250 [00:00<00:00, 68831.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "if existing_processed_datasets:\n",
    "    train_dataset = Dataset.load_from_disk(\"./ChartQA_processed_datasets/train_dataset\")\n",
    "    eval_dataset = Dataset.load_from_disk(\"./ChartQA_processed_datasets/eval_dataset\")\n",
    "    test_dataset = Dataset.load_from_disk(\"./ChartQA_processed_datasets/test_dataset\")\n",
    "\n",
    "else:\n",
    "    system_message = \"\"\"You are a Vision Language Model specialized in interpreting visual data from chart images.\n",
    "Your task is to analyze the provided chart image and respond to queries with concise answers, usually a single word, number, or short phrase.\n",
    "The charts include a variety of types (e.g., line charts, bar charts) and contain colors, labels, and text.\n",
    "Focus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.\"\"\"\n",
    "\n",
    "    dataset_id = \"HuggingFaceM4/ChartQA\"\n",
    "    train_dataset, eval_dataset, test_dataset = load_dataset(dataset_id, split=[\"train[:10%]\", \"val[:10%]\", \"test[:10%]\"])\n",
    "    \n",
    "    t0 = time.time()\n",
    "    filtered_count = sum(1 for sample in train_dataset if sample['image'] is None)\n",
    "    print(f\"Filtered out {filtered_count} samples with None image from train_dataset\")\n",
    "    train_dataset = [format_data(sample) for sample in train_dataset if sample['image'] is not None]\n",
    "    t1 = time.time()\n",
    "    print(\"time taken (train_dataset to list) : \", t1 - t0)\n",
    "    train_dataset = Dataset.from_list(train_dataset)\n",
    "    t2 = time.time()\n",
    "    print(\"train_dataset length: \", len(train_dataset))\n",
    "    print(\"time taken (train_dataset to Dataset) : \", t2 - t1)\n",
    "\n",
    "    eval_dataset = Dataset.from_list([format_data(sample) for sample in eval_dataset if sample['image'] is not None])\n",
    "    t3 = time.time()\n",
    "    print(\"eval_dataset length: \", len(eval_dataset))\n",
    "    print(\"time taken (eval_dataset to Dataset) : \", t3 - t2)\n",
    "\n",
    "    test_dataset = Dataset.from_list([format_data(sample) for sample in test_dataset if sample['image'] is not None])\n",
    "    t4 = time.time()\n",
    "    print(\"test_dataset length: \", len(test_dataset))\n",
    "    print(\"time taken (test_dataset to Dataset) : \", t4 - t3)\n",
    "    \n",
    "    train_dataset.save_to_disk(\"./ChartQA_processed_datasets/train_dataset\")\n",
    "    eval_dataset.save_to_disk(\"./ChartQA_processed_datasets/eval_dataset\")\n",
    "    test_dataset.save_to_disk(\"./ChartQA_processed_datasets/test_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e786b232",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.select(range(100))\n",
    "eval_dataset = eval_dataset.select(range(10))\n",
    "test_dataset = test_dataset.select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d7e7f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.\n",
      "Some kwargs in processor config are unused and will not have any effect: auto_map, image_start_token, image_placeholder, video_placeholder, image_end_token, tokens_per_tile. \n"
     ]
    }
   ],
   "source": [
    "model_id = \"nvidia/Eagle2-2B\"\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"nvidia/Eagle2-2B\", \n",
    "    trust_remote_code=True, \n",
    "    use_fast=True\n",
    ")\n",
    "processor.tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89ff8786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_from_sample(model, processor, sample, max_new_tokens=1024, device=\"cuda\"):\n",
    "    # Prepare the text input by applying the chat template\n",
    "    text_input = [processor.apply_chat_template(\n",
    "        sample[\"messages\"][:1], tokenize=False, add_generation_prompt=True\n",
    "    )]\n",
    "    \n",
    "    image_inputs, video_inputs = processor.process_vision_info(sample[\"messages\"][:1])\n",
    "        \n",
    "    # Prepare the inputs for the model\n",
    "    model_inputs = processor(\n",
    "        text=text_input,\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    ).to(\n",
    "        device\n",
    "    )  # Move inputs to the specified device\n",
    "    \n",
    "    model_inputs = model_inputs.to(device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Generate text with the model\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    # Decode the output text\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "\n",
    "    return output_text[0]  # Return the first decoded output text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28c8d919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = generate_text_from_sample(model, processor, train_dataset[1])\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82dc1da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "\n",
    "\n",
    "def clear_memory():\n",
    "    # Delete variables if they exist in the current global scope\n",
    "    if \"inputs\" in globals():\n",
    "        del globals()[\"inputs\"]\n",
    "    if \"model\" in globals():\n",
    "        del globals()[\"model\"]\n",
    "    if \"processor\" in globals():\n",
    "        del globals()[\"processor\"]\n",
    "    if \"trainer\" in globals():\n",
    "        del globals()[\"trainer\"]\n",
    "    if \"peft_model\" in globals():\n",
    "        del globals()[\"peft_model\"]\n",
    "    if \"bnb_config\" in globals():\n",
    "        del globals()[\"bnb_config\"]\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Garbage collection and clearing CUDA memory\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    time.sleep(2)\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "\n",
    "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c107f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: auto_map, image_start_token, image_placeholder, video_placeholder, image_end_token, tokens_per_tile. \n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={'': torch.cuda.current_device()} if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True, use_fast=True)\n",
    "processor.tokenizer.padding_side = \"left\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d8a09d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 43,639,936 || all params: 2,024,966,720 || trainable%: 2.1551\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"down_proj\", \"o_proj\", \"k_proj\", \"q_proj\", \"gate_proj\", \"up_proj\", \"v_proj\"],\n",
    "    use_dora=True,\n",
    "    init_lora_weights=\"gaussian\",\n",
    ")\n",
    "\n",
    "# Apply PEFT model adaptation\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07244ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/compu/anaconda3/envs/eagle2/lib/python3.10/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/home/compu/anaconda3/envs/eagle2/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 160.34 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_dataset columns: ['input_ids', 'attention_mask', 'pixel_values', 'image_flags', 'image_sizes', 'labels']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 190.13 examples/s]\n",
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_dataset columns: ['input_ids', 'attention_mask', 'pixel_values', 'image_flags', 'image_sizes', 'labels']\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "from eagle2_trl_sft_trainer import Eagle2TRLSFTTrainer\n",
    "from eagle2_data_collator import Eagle2DataCollator\n",
    "\n",
    "# Configure training arguments\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"eagle2-2b-trl-sft-ChartQA\",  # Directory to save the model\n",
    "    num_train_epochs=20,  # Number of training epochs\n",
    "    per_device_train_batch_size=1,  # Batch size for training\n",
    "    per_device_eval_batch_size=1,  # Batch size for evaluation\n",
    "    gradient_accumulation_steps=1,  # Steps to accumulate gradients\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Options for gradient checkpointing\n",
    "    max_length=None,\n",
    "    # Optimizer and scheduler settings\n",
    "    optim=\"adamw_torch_fused\",  # Optimizer type\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=2e-4,  # Learning rate for training\n",
    "    # Logging and evaluation\n",
    "    logging_steps=10,  # Steps interval for logging\n",
    "    eval_steps=50,  # Steps interval for evaluation\n",
    "    eval_strategy=\"steps\",  # Strategy for evaluation\n",
    "    save_strategy=\"steps\",  # Strategy for saving the model\n",
    "    save_steps=200,  # Steps interval for saving\n",
    "    # Mixed precision and gradient settings\n",
    "    bf16=True,  # Use bfloat16 precision\n",
    "    # max_grad_norm=0.3,  # Maximum norm for gradient clipping\n",
    "    warmup_ratio=0.03,  # Ratio of total steps for warmup\n",
    "    remove_unused_columns=False,  # Whether to remove unused columns\n",
    "    # Hub and reporting\n",
    "    push_to_hub=False,  # Whether to push model to Hugging Face Hub\n",
    "    use_legacy_prediction_loop=True,\n",
    "    report_to=\"wandb\",  # Use Weights & Biases for logging\n",
    ")\n",
    "\n",
    "processor.tokenizer.pad_token = \"<|endoftext|>\"\n",
    "processor.tokenizer.pad_token_id = 151643\n",
    "\n",
    "eagle2_data_collator = Eagle2DataCollator(processor.tokenizer)\n",
    "\n",
    "\n",
    "trainer = Eagle2TRLSFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor,\n",
    "    data_collator=eagle2_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d7d3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If distributed environment variables are not set, manually configure for single process\n",
    "if 'RANK' not in os.environ:\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'  # Use any available port\n",
    "    os.environ['RANK'] = '0'\n",
    "    os.environ['WORLD_SIZE'] = '1'\n",
    "    \n",
    "    # Initialize backend ('nccl' if GPU is available, otherwise 'gloo')\n",
    "    backend = 'nccl' if torch.cuda.is_available() else 'gloo'\n",
    "    dist.init_process_group(backend=backend, init_method='env://')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc891e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[trainer.py] get_train_dataloader: train_dataset.column_names: ['input_ids', 'attention_mask', 'pixel_values', 'image_flags', 'image_sizes', 'labels']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mschaeck\u001b[0m (\u001b[33mschaeck-dongguk-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/compu/test_suchae/EAGLE/test_suchae/wandb/run-20250904_171820-4e2fvmqs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/schaeck-dongguk-university/huggingface/runs/4e2fvmqs' target=\"_blank\">eagle2-2b-trl-sft-ChartQA</a></strong> to <a href='https://wandb.ai/schaeck-dongguk-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/schaeck-dongguk-university/huggingface' target=\"_blank\">https://wandb.ai/schaeck-dongguk-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/schaeck-dongguk-university/huggingface/runs/4e2fvmqs' target=\"_blank\">https://wandb.ai/schaeck-dongguk-university/huggingface/runs/4e2fvmqs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 31\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(5.5697, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 5.569736957550049\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='87' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  87/2000 00:28 < 10:47, 2.96 it/s, Epoch 0.86/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.291500</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 35\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(5.1323, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 10.702024459838867\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 40\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(4.3957, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 15.097679138183594\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 36\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(5.2998, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 20.397485733032227\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 32\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(5.4123, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 25.809741973876953\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 39\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(4.9480, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 30.757705688476562\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 37\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(4.7146, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 35.4722900390625\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 33\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(5.3976, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 40.869895935058594\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 34\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(5.6902, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 46.56007385253906\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 33\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(5.1344, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 51.69442367553711\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 35\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(5.0343, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 5.034346580505371\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 33\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(4.6649, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 9.699274063110352\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 34\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(4.8825, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 14.581775665283203\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 39\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(3.9165, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 18.498262405395508\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 40\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(3.7282, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 22.226436614990234\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 41\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(3.4501, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 25.676586151123047\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 49\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(2.7721, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 28.448673248291016\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 39\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(3.5843, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 32.03298568725586\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 44\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(3.5042, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 35.53714370727539\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 33\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(3.5042, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 39.04136657714844\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 39\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(2.9763, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 2.976318120956421\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 41\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(2.6390, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 5.615302085876465\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 32\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(2.7095, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 8.324843406677246\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 37\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(2.5676, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 10.892492294311523\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 44\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(3.1360, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 14.028461456298828\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 36\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(1.9412, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 15.969673156738281\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 35\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(2.3382, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 18.307886123657227\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 32\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(2.7206, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 21.028474807739258\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 40\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(1.8507, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 22.879127502441406\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 41\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(1.7605, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 24.639623641967773\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 34\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(1.7154, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 1.7154226303100586\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 37\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(2.0343, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 3.7497546672821045\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 35\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(1.7849, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 5.534679412841797\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 39\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(1.5165, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 7.051220893859863\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 52\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(1.6767, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 8.727957725524902\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 36\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(1.8782, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 10.606142044067383\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 45\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(2.5287, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 13.134799003601074\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 36\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(1.8300, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 14.964766502380371\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 52\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(2.1123, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 17.077102661132812\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 39\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(1.3167, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 18.39380645751953\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 34\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(1.3225, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 1.322515845298767\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 37\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(0.9430, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 2.2655322551727295\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 43\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(2.2309, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 4.496476173400879\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 28\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(0.9017, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 5.398200035095215\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 39\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(1.2274, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 6.625617027282715\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 45\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(1.3652, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 7.990806579589844\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 39\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(1.1084, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 9.09921646118164\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 44\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(2.0793, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 11.178472518920898\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 38\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(1.0110, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 12.18946361541748\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 32\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(0.7260, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 12.915495872497559\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "We are here in the prediction_loop\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/compu/anaconda3/envs/eagle2/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:477: FutureWarning: DistributedTensorGatherer is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 38\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 34\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 34\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 48\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 35\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 37\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 35\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 34\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 37\n",
      "[trainer.py] evaluate: output.metrics: {}\n",
      "[trainer.py] evaluate: output: EvalLoopOutput(predictions=None, label_ids=None, metrics={}, num_samples=10)\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 33\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(0.9148, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 0.9148154258728027\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 33\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(0.9296, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 1.8443872928619385\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 32\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(1.1609, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 3.0052924156188965\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 45\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(1.8324, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 4.8376617431640625\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 34\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(1.0658, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 5.903467178344727\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 38\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(0.6200, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 6.523461818695068\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 37\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(1.2287, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 7.752126693725586\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 32\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(0.4952, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 8.247329711914062\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 41\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(1.0538, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 9.30115795135498\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 32\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(0.8303, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 10.131465911865234\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 32\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(0.3808, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 0.38080793619155884\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 37\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(0.8295, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 1.2102994918823242\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 44\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(0.6278, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 1.838089942932129\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 40\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(1.0211, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 2.8591413497924805\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 40\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(1.1784, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 4.037506580352783\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 47\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(2.4911, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 6.528561592102051\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 43\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(1.2646, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 7.793173789978027\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 31\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(1.6541, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 9.447261810302734\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 38\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(1.2741, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 10.721342086791992\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 44\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(2.3341, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 13.055450439453125\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 33\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(0.9547, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 0.9547218084335327\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 40\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(1.0414, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 1.9961142539978027\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 39\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(1.2428, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 3.2388763427734375\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 43\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(1.6427, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 4.881529331207275\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 36\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(1.5434, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 6.424928665161133\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 30\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(0.8993, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 7.32424259185791\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 33\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(0.6214, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 7.945611953735352\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 44\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(0.8997, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 8.84526252746582\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 50\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(1.3514, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 10.19665241241455\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 33\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(0.8252, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 11.021834373474121\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 35\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(0.8737, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 0.8736786246299744\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 40\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(1.0958, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 1.969459056854248\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 33\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(1.3394, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 3.3088316917419434\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 35\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(0.9551, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 4.263965129852295\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 38\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(0.6799, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 4.943839073181152\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 37\n",
      "[trainer.py] _inner_training_loop: tr_loss_step tensor(0.6008, device='cuda:0')\n",
      "--------------------------------------------------\n",
      "Current Loss: 5.544638633728027\n",
      "Gradient for 'base_model.model.vision_model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight':\n",
      "  -> Grad Exists: True\n",
      "  -> Grad Mean: 0.0\n",
      "  -> Grad Max: 0.0\n",
      "--------------------------------------------------\n",
      "dynamic ViT batch size: 1, images per sample: 1.0, dynamic token length: 48\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "# [trainer.py] get_train_dataloader: train_dataset.column_names: ['input_ids', 'attention_mask', 'pixel_values', 'image_sizes', 'image_flags', 'labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671f9711",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b1d343",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f4205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac079c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\"nvidia/Eagle2-2B\",trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
    "processor = AutoProcessor.from_pretrained(\"nvidia/Eagle2-2B\", trust_remote_code=True, use_fast=True)\n",
    "processor.tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f216cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = \"/home/compu/test_suchae/EAGLE/test_suchae/eagle2-2b-trl-sft-ChartQA/checkpoint-260\"\n",
    "model.load_adapter(\"adapter_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6380c2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generate_text_from_sample(model, processor, train_dataset[0])\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eagle2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
