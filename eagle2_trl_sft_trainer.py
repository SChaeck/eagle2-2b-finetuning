import os
import io
import copy
import torch
import requests
from PIL import Image

from trl import SFTTrainer
from typing import Callable, Optional, Union
from datasets import Dataset, IterableDataset
from transformers import PreTrainedTokenizerBase, ProcessorMixin, BaseImageProcessor, FeatureExtractionMixin
from trl import SFTConfig
import warnings

# Define a new class that inherits from SFTTrainer.
class Eagle2TRLSFTTrainer(SFTTrainer):
    # Only override the _prepare_dataset method; keep other parts unchanged.
    def _prepare_dataset(
        self,
        dataset: Union[Dataset, IterableDataset],
        processing_class: Union[PreTrainedTokenizerBase, BaseImageProcessor, FeatureExtractionMixin, ProcessorMixin],
        args: SFTConfig,
        packing: bool,
        formatting_func: Optional[Callable[[dict], str]],
        dataset_name: str,
        newline_between_blocks: bool = True
    ) -> Union[Dataset, IterableDataset]:
        
        # If the dataset already contains 'input_ids', consider preprocessing done and do nothing.
        # This logic is the same as the original SFTTrainer.
        if "input_ids" in next(iter(dataset)).keys():
            return dataset

        # Implement the complex preprocessing logic for the Eagle model here.
        # This function will be used instead of the original tokenize function in SFTTrainer.
        def eagle_preprocess_function(sample):
            # Find the part in the messages structure that contains the image information.
            # This is based on the structure generated by format_data.
            sample_copy = copy.deepcopy(sample)
            has_any_image = False
            
            # Normalize messages reference
            messages = sample_copy["messages"]

            # Collect and normalize all images across all messages/contents
            pil_images = []
            image_inputs = None
            
            # Remove null values from content items
            for message in messages:
                if "content" in message:
                    for content_item in message["content"]:
                        # Remove keys with null values
                        keys_to_remove = [key for key, value in content_item.items() if value is None]
                        for key in keys_to_remove:
                            del content_item[key]
            
            # Walk through all content items to collect images
            for message in messages:
                if "content" not in message:
                    continue
                for content_item in message["content"]:
                    if content_item.get("type") == "image" and ("image" in content_item or "image_url" in content_item):
                        raw_image_data = content_item["image"] if "image" in content_item else content_item["image_url"]

                        pil_image = None
                        if isinstance(raw_image_data, Image.Image):
                            pil_image = raw_image_data
                        elif isinstance(raw_image_data, dict) and 'bytes' in raw_image_data and raw_image_data['bytes'] is not None:
                            image_bytes = raw_image_data['bytes']
                            pil_image = Image.open(io.BytesIO(image_bytes))
                        elif isinstance(raw_image_data, dict) and 'url' in raw_image_data and raw_image_data['url'] is not None:
                            image_url = raw_image_data['url']

                            # Check if the URL is a web address
                            if image_url.startswith('http://') or image_url.startswith('https://'):
                                try:
                                    response = requests.get(image_url)
                                    # Check if status code is 200 (success)
                                    if response.status_code == 200:
                                        pil_image = Image.open(io.BytesIO(response.content))
                                    else:
                                        print(f"Failed to load web image. Status code: {response.status_code}")
                                except requests.exceptions.RequestException as e:
                                    print(f"Error occurred during web image request: {e}")
                            # Check if the URL is a local file path
                            elif os.path.exists(image_url):
                                try:
                                    pil_image = Image.open(image_url)
                                except FileNotFoundError:
                                    print(f"Error: File does not exist at path {image_url}.")
                                except Exception as e:
                                    print(f"Error: Problem occurred while opening local image: {e}")
                        else:
                            # Unsupported type; skip this content item
                            pil_image = None

                        if pil_image is not None:
                            has_any_image = True
                            pil_image = pil_image.convert("RGB")
                            # Update the in-memory structure to hold the PIL image
                            content_item["image"] = pil_image
                            pil_images.append(pil_image)
                    else: # text
                        if newline_between_blocks and message["role"] == "user":
                            content_item["text"] = "\n" + content_item["text"] + " "
                            
            if has_any_image:
                # Let the processor build image_inputs (handles multi-view)
                image_inputs, video_inputs = processing_class.process_vision_info(messages)
                # Reflect normalized PIL images back to the original sample as well
                sample["messages"] = messages

            # Build prompt-only and full texts to later mask non-answer tokens in labels
            prompt_messages = copy.deepcopy(messages)
            if len(prompt_messages) > 0 and prompt_messages[-1].get("role") == "assistant":
                prompt_messages = prompt_messages[:-1]

            full_text = processing_class.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=False
            )
            prompt_text = processing_class.apply_chat_template(
                prompt_messages, tokenize=False, add_generation_prompt=True
            )
            
            text = [full_text]
                        
            # Process inputs based on whether we have images or not
            if has_any_image and image_inputs:
                # With one or more images
                model_inputs = processing_class(text=text, images=image_inputs, videos=None, return_tensors="pt")
                
                # image_flags should indicate number of views
                if "pixel_values" in model_inputs:
                    pixel_values = model_inputs["pixel_values"]
                    # Normalize shapes so that per-sample pixel_values is [N, C, H, W]
                    if isinstance(pixel_values, torch.Tensor):
                        if pixel_values.dim() == 5 and pixel_values.shape[0] == 1:
                            pixel_values = pixel_values.squeeze(0)
                        elif pixel_values.dim() == 3:
                            pixel_values = pixel_values.unsqueeze(0)
                    model_inputs["pixel_values"] = pixel_values

                    num_views = pixel_values.shape[0] if isinstance(pixel_values, torch.Tensor) else 1
                    model_inputs["image_flags"] = torch.ones(num_views, dtype=torch.bool)
            else:
                # Text-only case
                model_inputs = processing_class(text=text, images=None, videos=None, return_tensors="pt")
                
                # For text-only inputs, we need to ensure consistent tensor shapes
                # Add dummy pixel_values and image_flags if the model expects them
                if hasattr(processing_class, 'image_processor') and processing_class.image_processor is not None:
                    # Create dummy image tensors with proper shape
                    dummy_pixel_values = torch.zeros(1, 3, 448, 448)  # [N=1, C, H, W]
                    model_inputs["pixel_values"] = dummy_pixel_values
                    # All False for no real images
                    model_inputs["image_flags"] = torch.zeros(1, dtype=torch.bool)
                    
                    # Add dummy image_sizes if needed
                    if "image_sizes" not in model_inputs:
                        model_inputs["image_sizes"] = torch.tensor([[448, 448]])
                
            model_inputs["labels"] = model_inputs["input_ids"].clone()

            # Mask out prompt prefix tokens so only assistant answer contributes to loss
            try:
                prompt_tokenized = processing_class.tokenizer(
                    prompt_text,
                    return_tensors="pt"
                )
                prompt_len = prompt_tokenized.input_ids.shape[1]
                model_inputs["labels"][..., :prompt_len] = -100
            except Exception:
                pass

            # Additionally, mask image placeholder tokens in labels if present
            # Primary masking: model-specific image context token
            try:
                image_token_id = processing_class.tokenizer.convert_tokens_to_ids("<IMG_CONTEXT>")
            except Exception:
                image_token_id = 151667
            if isinstance(image_token_id, int) and image_token_id != -1:
                image_token_mask = model_inputs["input_ids"] == image_token_id
                model_inputs["labels"][image_token_mask] = -100

            # Secondary masking: explicit placeholders like <image-1>, <image-2>, <image>
            placeholders_to_try = ["<image>"] + [f"<image-{i}>" for i in range(1, 9)]
            for placeholder in placeholders_to_try:
                try:
                    pid = processing_class.tokenizer.convert_tokens_to_ids(placeholder)
                except Exception:
                    pid = None
                if isinstance(pid, int) and pid not in (-1, None):
                    mask = model_inputs["input_ids"] == pid
                    model_inputs["labels"][mask] = -100
            
            # Build processed dict with careful shape normalization
            processed = {}
            for k, v in model_inputs.items():
                if isinstance(v, torch.Tensor):
                    # text tensors: [1, L] -> [L]
                    if k in ["input_ids", "attention_mask", "labels"] and v.dim() == 2 and v.shape[0] == 1:
                        processed[k] = v.squeeze(0)
                    # pixel_values: prefer [N, C, H, W]
                    elif k == "pixel_values":
                        pv = v
                        if pv.dim() == 5 and pv.shape[0] == 1:
                            pv = pv.squeeze(0)
                        elif pv.dim() == 3:
                            pv = pv.unsqueeze(0)
                        processed[k] = pv
                    # image_sizes: prefer [N, 2]
                    elif k == "image_sizes":
                        sz = v
                        if sz.dim() == 3 and sz.shape[0] == 1:
                            sz = sz.squeeze(0)
                        elif sz.dim() == 1 and sz.numel() == 2:
                            sz = sz.unsqueeze(0)
                        processed[k] = sz
                    # image_flags: prefer [N]
                    elif k == "image_flags":
                        flags = v
                        if flags.dim() == 2 and flags.shape[0] == 1:
                            flags = flags.squeeze(0)
                        elif flags.dim() == 0:
                            flags = flags.unsqueeze(0)
                        processed[k] = flags
                    else:
                        # default behavior
                        processed[k] = v.squeeze(0) if (v.dim() > 1 and v.shape[0] == 1) else v
                else:
                    processed[k] = v
            
            return processed

        # Apply the above preprocessing function to the entire dataset.
        processed_dataset = dataset.map(
            eagle_preprocess_function,
            remove_columns=dataset.column_names,
            writer_batch_size=200  
        )
        
        processed_dataset.set_format(
            type='torch', 
            columns=['input_ids', 'attention_mask', 'pixel_values', 'image_sizes', 'labels', 'image_flags']
        )
        
        print("processed_dataset columns:", processed_dataset.column_names)
        
        # The following imports pack_dataset and truncate_dataset from the trl library.
        from trl.data_utils import pack_dataset, truncate_dataset

        map_kwargs = {}
        if isinstance(dataset, Dataset):
            map_kwargs["num_proc"] = args.dataset_num_proc

        if packing:
            if args.max_length is None:
                raise ValueError("When packing is enabled, `max_length` can't be `None`.")
            if isinstance(processed_dataset, Dataset):
                map_kwargs["desc"] = f"Packing {dataset_name} dataset"
            # Packing uses the columns: input_ids, attention_mask, and labels.
            # eagle_preprocess_function generates these columns, so it is compatible.
            processed_dataset = pack_dataset(processed_dataset, args.max_length, map_kwargs)
        elif args.max_length is not None:
            if isinstance(processed_dataset, Dataset):
                map_kwargs["desc"] = f"Truncating {dataset_name} dataset"
            processed_dataset = truncate_dataset(processed_dataset, args.max_length, map_kwargs)

        return processed_dataset