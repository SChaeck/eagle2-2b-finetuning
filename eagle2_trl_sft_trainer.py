import io
import copy
import torch
import requests
from PIL import Image

from trl import SFTTrainer
from typing import Callable, Optional, Union
from datasets import Dataset, IterableDataset
from transformers import PreTrainedTokenizerBase, ProcessorMixin, BaseImageProcessor, FeatureExtractionMixin
from trl import SFTConfig
import warnings

# Define a new class that inherits from SFTTrainer.
class Eagle2TRLSFTTrainer(SFTTrainer):
    # Only override the _prepare_dataset method; keep other parts unchanged.
    def _prepare_dataset(
        self,
        dataset: Union[Dataset, IterableDataset],
        processing_class: Union[PreTrainedTokenizerBase, BaseImageProcessor, FeatureExtractionMixin, ProcessorMixin],
        args: SFTConfig,
        packing: bool,
        formatting_func: Optional[Callable[[dict], str]],
        dataset_name: str,
    ) -> Union[Dataset, IterableDataset]:
        
        # If the dataset already contains 'input_ids', consider preprocessing done and do nothing.
        # This logic is the same as the original SFTTrainer.
        if "input_ids" in next(iter(dataset)).keys():
            return dataset

        # Implement the complex preprocessing logic for the Eagle model here.
        # This function will be used instead of the original tokenize function in SFTTrainer.
        def eagle_preprocess_function(sample):
            # Find the part in the messages structure that contains the image information.
            # This is based on the structure generated by format_data.
            sample_copy = copy.deepcopy(sample)
            image_content = None
            has_image = False
            
            # Check if there's an image in the messages
            for item in sample_copy["messages"][0]["content"]:
                if item.get("type") == "image":
                    image_content = item
                    has_image = True
                    break
            
            messages = sample_copy["messages"]

            pil_image = None
            image_inputs = None
            
            # Remove null values from content items
            for message in messages:
                if "content" in message:
                    for content_item in message["content"]:
                        # Remove keys with null values
                        keys_to_remove = [key for key, value in content_item.items() if value is None]
                        for key in keys_to_remove:
                            del content_item[key]
            
            if has_image and image_content:
                raw_image_data = image_content["image"]
                
                # Case 1: The data is already a PIL Image object (when used directly from memory)
                if isinstance(raw_image_data, Image.Image):
                    pil_image = raw_image_data
                # Case 2: The data is a serialized dict (when loaded from disk)
                elif isinstance(raw_image_data, dict) and 'bytes' in raw_image_data and raw_image_data['bytes'] is not None:
                    image_bytes = raw_image_data['bytes']
                    pil_image = Image.open(io.BytesIO(image_bytes))
                else:
                    print(f"Warning: Unsupported image data type: {type(raw_image_data)}, treating as text-only")
                    has_image = False
                
                if pil_image:
                    # Convert to RGB mode for consistency
                    pil_image = pil_image.convert("RGB")
                    messages[0]["content"][0]["image"] = pil_image
                    
                    # print("!@#$messages:", messages)
                    
                    image_inputs, video_inputs = processing_class.process_vision_info(messages)
                    
                    # Update the sample with the converted PIL Image object
                    sample["messages"][0]["content"][0]["image"] = pil_image

            # Generate text using chat template
            text = [processing_class.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=False
            )]
            
            # print("!@#$text:", text)
            # print("!@#$image_inputs:", image_inputs)
            
            # Process inputs based on whether we have images or not
            if has_image and image_inputs:
                # With image
                model_inputs = processing_class(text=text, images=image_inputs, videos=None, return_tensors="pt")
                
                # Pad dynamic batch size to [1, 3, 448, 448] (or whatever target size)
                if "pixel_values" in model_inputs:
                    target_batch_size = 1
                   
                    # Since all images are real images, fill with True
                    model_inputs["image_flags"] = torch.ones(target_batch_size, dtype=torch.bool)
            else:
                # Text-only case
                model_inputs = processing_class(text=text, images=None, videos=None, return_tensors="pt")
                
                # For text-only inputs, we need to ensure consistent tensor shapes
                # Add dummy pixel_values and image_flags if the model expects them
                if hasattr(processing_class, 'image_processor') and processing_class.image_processor is not None:
                    # Create dummy image tensors with proper shape
                    dummy_pixel_values = torch.zeros(1, 3, 448, 448)  # Adjust size as needed
                    model_inputs["pixel_values"] = dummy_pixel_values
                    # All False for no real images
                    model_inputs["image_flags"] = torch.zeros(1, dtype=torch.bool)
                    
                    # Add dummy image_sizes if needed
                    if "image_sizes" not in model_inputs:
                        model_inputs["image_sizes"] = torch.tensor([[448, 448]])
                
            model_inputs["labels"] = model_inputs["input_ids"].clone()
            
            processed = {}
            for k, v in model_inputs.items():
                if isinstance(v, torch.Tensor):
                    if v.dim() > 1:
                        processed[k] = v.squeeze(0)
                    else:
                        processed[k] = v
                else:
                    processed[k] = v
            
            return processed

        # Apply the above preprocessing function to the entire dataset.
        processed_dataset = dataset.map(
            eagle_preprocess_function,
            remove_columns=dataset.column_names,
            writer_batch_size=200  
        )
        
        processed_dataset.set_format(
            type='torch', 
            columns=['input_ids', 'attention_mask', 'pixel_values', 'image_sizes', 'labels', 'image_flags']
        )
        
        print("processed_dataset columns:", processed_dataset.column_names)
        
        # The following imports pack_dataset and truncate_dataset from the trl library.
        from trl.data_utils import pack_dataset, truncate_dataset

        map_kwargs = {}
        if isinstance(dataset, Dataset):
            map_kwargs["num_proc"] = args.dataset_num_proc

        if packing:
            if args.max_length is None:
                raise ValueError("When packing is enabled, `max_length` can't be `None`.")
            if isinstance(processed_dataset, Dataset):
                map_kwargs["desc"] = f"Packing {dataset_name} dataset"
            # Packing uses the columns: input_ids, attention_mask, and labels.
            # eagle_preprocess_function generates these columns, so it is compatible.
            processed_dataset = pack_dataset(processed_dataset, args.max_length, map_kwargs)
        elif args.max_length is not None:
            if isinstance(processed_dataset, Dataset):
                map_kwargs["desc"] = f"Truncating {dataset_name} dataset"
            processed_dataset = truncate_dataset(processed_dataset, args.max_length, map_kwargs)

        return processed_dataset