{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "532dfe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"You are a Vision Language Model specialized in interpreting visual data from chart images.\n",
    "Your task is to analyze the provided chart image and respond to queries with concise answers, usually a single word, number, or short phrase.\n",
    "The charts include a variety of types (e.g., line charts, bar charts) and contain colors, labels, and text.\n",
    "Focus on delivering accurate, succinct answers based on the visual information. Avoid additional explanation unless absolutely necessary.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cedb54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(sample):\n",
    "    return {\n",
    "        \"images\": [sample[\"image\"]],\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"image\": sample[\"image\"],\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": sample[\"query\"],\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": sample[\"label\"][0]}],\n",
    "            },\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddfca4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/compu/anaconda3/envs/eagle2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_id = \"HuggingFaceM4/ChartQA\"\n",
    "train_dataset, eval_dataset, test_dataset = load_dataset(dataset_id, split=[\"train[:10%]\", \"val[:10%]\", \"test[:10%]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49cc3f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = [format_data(sample) for sample in train_dataset]\n",
    "eval_dataset = [format_data(sample) for sample in eval_dataset]\n",
    "test_dataset = [format_data(sample) for sample in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e11579f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
    "\n",
    "model_id = \"Qwen/Qwen2-VL-7B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9353858b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [02:05<00:00, 25.02s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:02<00:00,  1.92it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "processor = Qwen2VLProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddd723a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "\n",
    "def generate_text_from_sample(model, processor, sample, max_new_tokens=1024, device=\"cuda\"):\n",
    "    # Prepare the text input by applying the chat template\n",
    "    text_input = processor.apply_chat_template(\n",
    "        sample[\"messages\"][1:2],  # Use the sample without the system message\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    # Process the visual input from the sample\n",
    "    image_inputs, _ = process_vision_info(sample[\"messages\"])\n",
    "\n",
    "    # Prepare the inputs for the model\n",
    "    model_inputs = processor(\n",
    "        text=[text_input],\n",
    "        images=image_inputs,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\n",
    "        device\n",
    "    )  # Move inputs to the specified device\n",
    "\n",
    "    # Generate text with the model\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    # Trim the generated ids to remove the input ids\n",
    "    trimmed_generated_ids = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "\n",
    "    # Decode the output text\n",
    "    output_text = processor.batch_decode(\n",
    "        trimmed_generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "\n",
    "    return output_text[0]  # Return the first decoded output text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4ec9314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No, the value of Favorable is not 38 in 2015. According to the chart, the value of Favorable in 2015 is 38.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of how to call the method with sample:\n",
    "output = generate_text_from_sample(model, processor, train_dataset[0])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "505ff3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU allocated memory: 0.03 GB\n",
      "GPU reserved memory: 0.03 GB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import time\n",
    "\n",
    "\n",
    "def clear_memory():\n",
    "    # Delete variables if they exist in the current global scope\n",
    "    if \"inputs\" in globals():\n",
    "        del globals()[\"inputs\"]\n",
    "    if \"model\" in globals():\n",
    "        del globals()[\"model\"]\n",
    "    if \"processor\" in globals():\n",
    "        del globals()[\"processor\"]\n",
    "    if \"trainer\" in globals():\n",
    "        del globals()[\"trainer\"]\n",
    "    if \"peft_model\" in globals():\n",
    "        del globals()[\"peft_model\"]\n",
    "    if \"bnb_config\" in globals():\n",
    "        del globals()[\"bnb_config\"]\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Garbage collection and clearing CUDA memory\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    time.sleep(2)\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "\n",
    "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6ed23a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_id, device_map=\"auto\", torch_dtype=torch.bfloat16, quantization_config=bnb_config\n",
    ")\n",
    "processor = Qwen2VLProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d784d2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,523,136 || all params: 8,293,898,752 || trainable%: 0.0304\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply PEFT model adaptation\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a1d1ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "# Configure training arguments\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"qwen2-7b-instruct-trl-sft-ChartQA\",  # Directory to save the model\n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    per_device_train_batch_size=4,  # Batch size for training\n",
    "    per_device_eval_batch_size=4,  # Batch size for evaluation\n",
    "    gradient_accumulation_steps=8,  # Steps to accumulate gradients\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Options for gradient checkpointing\n",
    "    max_length=None,\n",
    "    # Optimizer and scheduler settings\n",
    "    optim=\"adamw_torch_fused\",  # Optimizer type\n",
    "    learning_rate=2e-4,  # Learning rate for training\n",
    "    # Logging and evaluation\n",
    "    logging_steps=10,  # Steps interval for logging\n",
    "    eval_steps=10,  # Steps interval for evaluation\n",
    "    eval_strategy=\"steps\",  # Strategy for evaluation\n",
    "    save_strategy=\"steps\",  # Strategy for saving the model\n",
    "    save_steps=20,  # Steps interval for saving\n",
    "    # Mixed precision and gradient settings\n",
    "    bf16=True,  # Use bfloat16 precision\n",
    "    max_grad_norm=0.3,  # Maximum norm for gradient clipping\n",
    "    warmup_ratio=0.03,  # Ratio of total steps for warmup\n",
    "    # Hub and reporting\n",
    "    push_to_hub=True,  # Whether to push model to Hugging Face Hub\n",
    "    report_to=\"wandb\",  # Reporting tool for tracking metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83d1c518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/compu/anaconda3/envs/eagle2/lib/python3.10/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/home/compu/anaconda3/envs/eagle2/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtrl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SFTTrainer\n\u001b[0;32m----> 3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/eagle2/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:350\u001b[0m, in \u001b[0;36mSFTTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletion_only_loss \u001b[38;5;129;01mand\u001b[39;00m formatting_func:\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA formatting function was provided while `completion_only_loss=True`, which is incompatible. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a formatter converts the dataset to a language modeling type, conflicting with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion-only loss. To resolve this, apply your formatting function before passing the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    347\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset, or disable `completion_only_loss` in `SFTConfig`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    348\u001b[0m     )\n\u001b[0;32m--> 350\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpacking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    352\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m     packing \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mpacking \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39meval_packing \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m args\u001b[38;5;241m.\u001b[39meval_packing\n",
      "File \u001b[0;32m~/anaconda3/envs/eagle2/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:567\u001b[0m, in \u001b[0;36mSFTTrainer._prepare_dataset\u001b[0;34m(self, dataset, processing_class, args, packing, formatting_func, dataset_name)\u001b[0m\n\u001b[1;32m    565\u001b[0m     map_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdesc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dataset to ChatML\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m column_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dataset))\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m--> 567\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m(\n\u001b[1;32m    568\u001b[0m     maybe_convert_to_chatml,\n\u001b[1;32m    569\u001b[0m     remove_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconversations\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconversations\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m column_names \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmap_kwargs,\n\u001b[1;32m    571\u001b[0m )\n\u001b[1;32m    573\u001b[0m \u001b[38;5;66;03m# Apply the chat template if needed\u001b[39;00m\n\u001b[1;32m    574\u001b[0m first_example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dataset))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8081bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eagle2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
